{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import list_datasets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Save the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_dataset(dataset_name, subset=None, batch_size=1000):\n",
    "    # Load the dataset\n",
    "    if subset:\n",
    "        dataset = load_dataset(dataset_name, subset=subset, split='train', streaming=True)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_name, split='train', streaming=True)\n",
    "\n",
    "    # Specify the file where you want to save the dataset\n",
    "    # Change the path to a directory where you have write permissions\n",
    "    save_file = f\"./data/{dataset_name}_dataset.jsonl\"  # Changed from /backend/data to ./data\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "\n",
    "    # Open the file in append mode\n",
    "    with open(save_file, 'a') as f:\n",
    "        batch = []\n",
    "        for example in dataset:\n",
    "            batch.append(example)\n",
    "            if len(batch) == batch_size:\n",
    "                # Write the batch to the file\n",
    "                for item in batch:\n",
    "                    f.write(json.dumps(item) + \"\\n\")\n",
    "                batch = []  # Clear the batch\n",
    "\n",
    "        # Write any remaining examples in the last batch\n",
    "        if batch:\n",
    "            for item in batch:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(f\"Dataset saved to {save_file}\")\n",
    "# List of datasets with size information in comments\n",
    "\"\"\"\n",
    "Dataset Descriptions:\n",
    "- tiny_shakespeare (~450 KB): A very small dataset ideal for testing or experimentation.\n",
    "- bookcorpus (~6 GB): Text extracted from books, used in LLM pretraining.\n",
    "- wikipedia (subset: 20220301.en, ~16 GB): English Wikipedia dump as of March 2022.\n",
    "- openwebtext (~40 GB): Open-source recreation of the WebText dataset.\n",
    "- the_pile (~825 GB): A massive dataset designed for large language model training.\n",
    "- c4 (subset: en, ~750 GB): Cleaned version of Common Crawl, frequently used for large-scale LLM training.\n",
    "- oscar (subset: unshuffled_deduplicated_en, ~1.3 TB): A multilingual web corpus.\n",
    "- common_crawl (Several TBs): Raw web crawl data; storage needs depend on the specific subset downloaded.\n",
    "\"\"\"\n",
    "datasets_to_download = [\n",
    "    # {\"name\": \"tiny_shakespeare\"},  # ~450 KB\n",
    "    {\"name\": \"bookcorpus\"},       # ~6 GB\n",
    "    # {\"name\": \"wikipedia\", \"subset\": \"20220301.en\"},  # ~16 GB\n",
    "    # {\"name\": \"openwebtext\"},      # ~40 GB\n",
    "    # {\"name\": \"the_pile\"},         # ~825 GB\n",
    "    # {\"name\": \"c4\", \"subset\": \"en\"},  # ~750 GB\n",
    "    # {\"name\": \"oscar\", \"subset\": \"unshuffled_deduplicated_en\"},  # ~1.3 TB\n",
    "    # {\"name\": \"common_crawl\"}      # Several TBs\n",
    "]\n",
    "\n",
    "\n",
    "# Loop through the datasets and download them\n",
    "for dataset_info in datasets_to_download:\n",
    "    name = dataset_info[\"name\"]\n",
    "    subset = dataset_info.get(\"subset\")  # Subset is optional\n",
    "    download_and_save_dataset(name, subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List all available datasets\n",
    "datasets = list(list_datasets())\n",
    "print(f\"Total datasets available: {len(datasets)}\")\n",
    "print(\"Some example datasets:\")\n",
    "for dataset in datasets[:10]:\n",
    "    print(dataset.id)\n",
    "\n",
    "# Prompt user to select a dataset\n",
    "selected_dataset = input(\"Enter the ID of the dataset you want to download: \")\n",
    "\n",
    "# Function to download the selected dataset\n",
    "def download_selected_dataset(dataset_id: str):\n",
    "    print(f\"Downloading dataset: {dataset_id}\")\n",
    "    dataset = load_dataset(dataset_id)\n",
    "    return dataset\n",
    "\n",
    "# Download the selected dataset\n",
    "downloaded_dataset = download_selected_dataset(selected_dataset)\n",
    "\n",
    "def save_dataset_incrementally(dataset_name: str, split: str, save_path: str, num_samples: int = 100):\n",
    "    \"\"\"Download and save a dataset incrementally to avoid data loss on errors.\"\"\"\n",
    "    dataset = load_dataset(dataset_name, split=split, streaming=True)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(dataset.take(num_samples), total=num_samples, desc=f\"Saving {dataset_name}\")):\n",
    "        with open(os.path.join(save_path, f\"{dataset_name}_sample_{i}.json\"), \"w\") as f:\n",
    "            json.dump(sample, f)\n",
    "        print(f\"Saved {dataset_name} sample {i+1}/{num_samples}\")\n",
    "\n",
    "# Load datasets from data directory\n",
    "data_dir = os.path.join(\"data\")\n",
    "\n",
    "# Save BookCorpus dataset incrementally\n",
    "books_path = os.path.join(data_dir, \"bookcorpus\")\n",
    "save_dataset_incrementally(\"bookcorpus\", \"train\", books_path)\n",
    "\n",
    "# Save Wikipedia dataset incrementally\n",
    "wiki_path = os.path.join(data_dir, \"wikipedia\")\n",
    "save_dataset_incrementally(\"wikipedia\", \"20220301.en\", wiki_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset Hashes\n",
    "This section uses Merkle hashes to generate dataset hashes and saves them in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_content(content: str) -> str:\n",
    "    \"\"\"Hash a string using SHA-256\"\"\"\n",
    "    return hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "def create_merkle_tree(data_list: List[str]) -> str:\n",
    "    \"\"\"Create a Merkle tree from a list of data and return root hash\"\"\"\n",
    "    if not data_list:\n",
    "        return hash_content(\"\")\n",
    "    \n",
    "    # Create leaf nodes by hashing each piece of data\n",
    "    hashes = [hash_content(str(data)) for data in data_list]\n",
    "    \n",
    "    # Build tree bottom-up until we reach root\n",
    "    while len(hashes) > 1:\n",
    "        if len(hashes) % 2 == 1:\n",
    "            hashes.append(hashes[-1])  # Duplicate last hash if odd number\n",
    "        \n",
    "        next_level = []\n",
    "        for i in range(0, len(hashes), 2):\n",
    "            combined = hashes[i] + hashes[i+1]\n",
    "            next_level.append(hash_content(combined))\n",
    "        hashes = next_level\n",
    "        \n",
    "    return hashes[0]  # Return root hash\n",
    "\n",
    "# Load and hash Wikipedia dataset\n",
    "wiki_data = load_dataset_from_directory(wiki_path)\n",
    "wiki_hash = create_merkle_tree(wiki_data)\n",
    "print(f\"Wikipedia dataset hash: {wiki_hash}\")\n",
    "\n",
    "# Load and hash BookCorpus dataset\n",
    "books_data = load_dataset_from_directory(books_path)\n",
    "books_hash = create_merkle_tree(books_data)\n",
    "print(f\"BookCorpus dataset hash: {books_hash}\")\n",
    "\n",
    "# Save dataset hashes to a JSON file\n",
    "dataset_hashes = {\n",
    "    \"wikipedia\": wiki_hash,\n",
    "    \"bookcorpus\": books_hash\n",
    "}\n",
    "\n",
    "hash_file_path = os.path.join(\"data\", \"dataset_hashes.json\")\n",
    "with open(hash_file_path, \"w\") as f:\n",
    "    json.dump(dataset_hashes, f, indent=4)\n",
    "\n",
    "print(f\"Dataset hashes saved to: {hash_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zk-ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
