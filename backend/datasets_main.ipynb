{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Save the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_directory(directory_path: str) -> List[dict]:\n",
    "    \"\"\"Load JSON files from a directory into a list\"\"\"\n",
    "    dataset = []\n",
    "    if os.path.exists(directory_path):\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith('.json'):\n",
    "                with open(os.path.join(directory_path, filename), 'r') as f:\n",
    "                    dataset.append(json.load(f))\n",
    "    return dataset\n",
    "\n",
    "# Load datasets from data directory\n",
    "data_dir = os.path.join(\"data\")\n",
    "\n",
    "# Load Wikipedia dataset\n",
    "wiki_path = os.path.join(data_dir, \"wikipedia\")\n",
    "wiki_dataset = load_dataset_from_directory(wiki_path)\n",
    "print(f\"Loaded {len(wiki_dataset)} Wikipedia articles\")\n",
    "\n",
    "# Load BookCorpus dataset\n",
    "books_path = os.path.join(data_dir, \"bookcorpus\")\n",
    "books_dataset = load_dataset_from_directory(books_path)\n",
    "print(f\"Loaded {len(books_dataset)} books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_content(content: str) -> str:\n",
    "    \"\"\"Hash a string using SHA-256\"\"\"\n",
    "    return hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "def create_merkle_tree(data_list: List[str]) -> str:\n",
    "    \"\"\"Create a Merkle tree from a list of data and return root hash\"\"\"\n",
    "    if not data_list:\n",
    "        return hash_content(\"\")\n",
    "    \n",
    "    # Create leaf nodes by hashing each piece of data\n",
    "    hashes = [hash_content(str(data)) for data in data_list]\n",
    "    \n",
    "    # Build tree bottom-up until we reach root\n",
    "    while len(hashes) > 1:\n",
    "        if len(hashes) % 2 == 1:\n",
    "            hashes.append(hashes[-1])  # Duplicate last hash if odd number\n",
    "        \n",
    "        next_level = []\n",
    "        for i in range(0, len(hashes), 2):\n",
    "            combined = hashes[i] + hashes[i+1]\n",
    "            next_level.append(hash_content(combined))\n",
    "        hashes = next_level\n",
    "        \n",
    "    return hashes[0]  # Return root hash\n",
    "\n",
    "# Load and hash Wikipedia dataset\n",
    "wiki_data = load_dataset_from_directory(wiki_path)\n",
    "wiki_hash = create_merkle_tree(wiki_data)\n",
    "print(f\"Wikipedia dataset hash: {wiki_hash}\")\n",
    "\n",
    "# Load and hash BookCorpus dataset\n",
    "books_data = load_dataset_from_directory(books_path)\n",
    "books_hash = create_merkle_tree(books_data)\n",
    "print(f\"BookCorpus dataset hash: {books_hash}\")\n",
    "\n",
    "# Save dataset hashes to a JSON file\n",
    "dataset_hashes = {\n",
    "    \"wikipedia\": wiki_hash,\n",
    "    \"bookcorpus\": books_hash\n",
    "}\n",
    "\n",
    "hash_file_path = os.path.join(\"data\", \"dataset_hashes.json\")\n",
    "with open(hash_file_path, \"w\") as f:\n",
    "    json.dump(dataset_hashes, f, indent=4)\n",
    "\n",
    "print(f\"Dataset hashes saved to: {hash_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zk-ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
